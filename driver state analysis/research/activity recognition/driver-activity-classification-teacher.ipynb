{"cells":[{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"!pip install --upgrade albumentations -q\n!pip install gdown\n\nimport os\nimport pandas as pd\nimport pickle\nimport shutil\nimport numpy as np\nimport seaborn as sns\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\nfrom PIL import ImageFile\nimport glob\nimport shutil\nimport cv2\n\nfrom sklearn.datasets import load_files\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score,precision_score,recall_score,f1_score\nfrom sklearn.model_selection import train_test_split\n\nimport tensorflow as tf\nfrom tensorflow.keras import utils\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D\nfrom tensorflow.keras.layers import Dropout, Flatten, Dense\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.utils import plot_model\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.preprocessing import image  \n\nimport albumentations as A\nfrom tensorflow.keras.models import Model\nimport tensorflow.keras.layers\nfrom tensorflow.keras.layers import Dense, ReLU\nfrom tensorflow.keras.layers import Dropout, BatchNormalization\nfrom tensorflow.keras.layers import GlobalAveragePooling2D\nfrom tensorflow.keras.models import load_model\nfrom tensorflow.keras.optimizers import Adam, SGD\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nfrom tensorflow.keras.optimizers.schedules import PolynomialDecay\nfrom tensorflow.keras.applications.inception_v3 import preprocess_input\nfrom tensorflow.keras.applications import EfficientNetB4\nfrom tensorflow.keras.models import load_model","execution_count":1,"outputs":[{"output_type":"stream","text":"Collecting gdown\n  Downloading gdown-3.12.2.tar.gz (8.2 kB)\n  Installing build dependencies ... \u001b[?25ldone\n\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n\u001b[?25h    Preparing wheel metadata ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from gdown) (1.15.0)\nRequirement already satisfied: requests[socks] in /opt/conda/lib/python3.7/site-packages (from gdown) (2.25.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from gdown) (3.0.12)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from gdown) (4.55.1)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests[socks]->gdown) (1.26.2)\nRequirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests[socks]->gdown) (3.0.4)\nRequirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests[socks]->gdown) (2.10)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests[socks]->gdown) (2020.12.5)\nRequirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /opt/conda/lib/python3.7/site-packages (from requests[socks]->gdown) (1.7.1)\nBuilding wheels for collected packages: gdown\n  Building wheel for gdown (PEP 517) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for gdown: filename=gdown-3.12.2-py3-none-any.whl size=9681 sha256=937dff78f1a7a62af1d810ebf9f523989541e55dc7e5813fce35e4db67f40818\n  Stored in directory: /root/.cache/pip/wheels/ba/e0/7e/726e872a53f7358b4b96a9975b04e98113b005cd8609a63abc\nSuccessfully built gdown\nInstalling collected packages: gdown\nSuccessfully installed gdown-3.12.2\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"def draw_plot_from_history(history, metric, n_epochs, stage):\n    plt.style.use(\"ggplot\")\n    plt.figure(figsize=(9, 5))\n    plt.plot(np.arange(0, n_epochs), history.history[metric], label=\"train_%s\" % metric)\n    if \"val_%s\" % metric in history.history:\n        plt.plot(np.arange(0, n_epochs), history.history[\"val_%s\" % metric], label=\"val_%s\" % metric)\n    title = stage + '_%s' % metric\n    plt.title(title)\n    plt.xlabel(\"Epoch #\")\n    plt.ylabel(\"Value\")\n    plt.legend()\n    filename = title + '.png'\n    plt.show()","execution_count":4,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class DataGenerator(tf.keras.utils.Sequence):\n    def __init__(self, data, batch_size, augmentation, image_size, shuffle=True):\n        self.batch_size = batch_size\n        self.augmentation = augmentation\n        self.shuffle = shuffle\n        self.image_size = image_size\n\n        self.image_names = np.array([item['path'] for item in data])\n        self.targets = to_categorical(np.array([int(item['label'][1:]) for item in data]))\n        self.samples = len(self.targets)\n\n        self.indexes = np.arange(self.samples)\n        if shuffle:\n            np.random.shuffle(self.indexes)\n\n    def __len__(self):\n        return int(np.ceil(self.samples / self.batch_size))\n    \n    def on_epoch_end(self):\n        if self.shuffle:\n            np.random.shuffle(self.indexes)\n\n    def transform_image(self, image):\n        image = cv2.resize(image, (self.image_size, self.image_size), interpolation = cv2.INTER_CUBIC)\n\n        if self.augmentation:\n            image = self.augmentation(image=image)['image']\n\n        return image\n\n    def __getitem__(self, index):\n        take_ind = self.indexes[index * self.batch_size: min((index + 1) * self.batch_size, len(self.targets))]\n        X = np.empty((len(take_ind), self.image_size, self.image_size, 3))\n        y = self.targets[take_ind, :]\n\n        for i in range(len(take_ind)):\n            img = cv2.imread(self.image_names[take_ind[i]], cv2.IMREAD_COLOR)\n            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n            img = self.transform_image(img)\n            X[i] = img\n\n        return X, y","execution_count":5,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Create augmentations:"},{"metadata":{"trusted":true},"cell_type":"code","source":"transform = A.Compose(\n    [\n        A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.2, rotate_limit=30, \n                           p=0.5, border_mode=cv2.BORDER_CONSTANT), \n        A.RandomBrightnessContrast(p=0.5),\n        A.RGBShift(p=0.25),\n        A.GaussNoise(p=0.25),\n        A.HorizontalFlip(p=0.5),\n    ]\n)","execution_count":9,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Create generators:"},{"metadata":{"trusted":true},"cell_type":"code","source":"TRAIN_DIR = \"/kaggle/input/state-farm-distracted-driver-detection/imgs/train\"\nTEST_DIR = \"/kaggle/input/state-farm-distracted-driver-detection/imgs/test\"","execution_count":6,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"INPUT_SHAPE = 380\nBATCH_SIZE = 32\nnum_classes = 10","execution_count":7,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data = []\nlabel_stat = []\n\nfor label in os.listdir(TRAIN_DIR):\n    for img_path in glob.glob(os.path.join(TRAIN_DIR, label, \"*.jpg\")):\n        train_data.append({'path': img_path, 'label': label})\n        label_stat.append(label)\n\ntrain_data, val_data = train_test_split(train_data, test_size=0.2, stratify=label_stat, shuffle=True)\n\ntrain_generator = DataGenerator(train_data, BATCH_SIZE, transform, INPUT_SHAPE)\nvalidation_generator = DataGenerator(val_data, BATCH_SIZE, None, INPUT_SHAPE, shuffle=False)","execution_count":10,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# train_generator.augmentation = transform","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Build model:"},{"metadata":{"trusted":true},"cell_type":"code","source":"model = EfficientNetB4(weights='imagenet', include_top=False, input_shape=(INPUT_SHAPE, INPUT_SHAPE, 3))\n\nfor layer in model.layers:\n    layer.trainable = False\n\nx = model.output\nx = GlobalAveragePooling2D()(x)\n# x = Dense(64, activation='relu')(x)\nx = Dropout(0.2)(x)\npredictions = Dense(num_classes, activation='softmax')(x)\n\nmodel = Model(inputs=model.input, outputs=predictions)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Warmup:"},{"metadata":{"trusted":true},"cell_type":"code","source":"NUM_EPOCHS = 5\nLEARNING_RATE = 0.001","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"opt = Adam(learning_rate=LEARNING_RATE)\nmodel.compile(optimizer=opt, loss='categorical_crossentropy', metrics=[\"accuracy\"])\n\nhistory = model.fit(\n    train_generator,\n    steps_per_epoch = train_generator.samples // BATCH_SIZE,\n    validation_data = validation_generator,\n    validation_steps = validation_generator.samples // BATCH_SIZE,\n    epochs=NUM_EPOCHS, verbose=1)\n\ndraw_plot_from_history(history, 'loss', NUM_EPOCHS, 'STAGE_1')\ndraw_plot_from_history(history, 'accuracy', NUM_EPOCHS, 'STAGE_1')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.save(\"model1_stage1.hdf5\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Unfreezeing deeper layers:"},{"metadata":{"trusted":true},"cell_type":"code","source":"NUM_EPOCHS = 30\nLEARNING_RATE = 0.0003\nFINE_TUNE_FROM_LAYER = -20","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for layer in model.layers:\n    layer.trainable = False\nfor layer in model.layers[FINE_TUNE_FROM_LAYER:]:\n    if not isinstance(layer, tensorflow.keras.layers.BatchNormalization):\n        layer.trainable = True","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learning_rate_fn = tf.keras.optimizers.schedules.PolynomialDecay(\n    initial_learning_rate=LEARNING_RATE,\n    decay_steps=NUM_EPOCHS * train_generator.samples // BATCH_SIZE,\n    end_learning_rate=LEARNING_RATE / 10,\n    power=1.0)\n\nopt = Adam(learning_rate=learning_rate_fn)\nmodel.compile(optimizer=opt, loss='categorical_crossentropy', metrics=[\"accuracy\"])\n\nsave_best_checkpoint = ModelCheckpoint(\"model1_stage2.hdf5\", verbose=1, monitor='val_loss', save_best_only=True, mode='auto')\n\nhistory = model.fit_generator(\n    train_generator,\n    steps_per_epoch = train_generator.samples // BATCH_SIZE,\n    validation_data = validation_generator,\n    validation_steps = validation_generator.samples // BATCH_SIZE,\n    epochs = NUM_EPOCHS,\n    callbacks=[save_best_checkpoint], \n    verbose=1)\n\ndraw_plot_from_history(history, 'loss', NUM_EPOCHS, 'STAGE_2')\ndraw_plot_from_history(history, 'accuracy', NUM_EPOCHS, 'STAGE_2')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def save_plot_from_history(history, metric, n_epochs, stage):\n    plt.style.use(\"ggplot\")\n    plt.figure(figsize=(21, 15))\n    plt.plot(np.arange(0, n_epochs), history.history[metric], label=\"train_%s\" % metric)\n    if \"val_%s\" % metric in history.history:\n        plt.plot(np.arange(0, n_epochs), history.history[\"val_%s\" % metric], label=\"val_%s\" % metric)\n    title = metric\n    plt.title(title, fontsize=18)\n    plt.xlabel(\"Epoch #\", fontsize=18)\n    plt.ylabel(\"Value\", fontsize=18)\n    plt.legend(fontsize=18)\n    plt.xticks(fontsize=14)\n    plt.yticks(fontsize=14)\n    filename = title + '.png'\n    plt.savefig(filename)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"save_plot_from_history(history, 'loss', NUM_EPOCHS, 'STAGE_2')\nsave_plot_from_history(history, 'accuracy', NUM_EPOCHS, 'STAGE_2')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Make predictions:"},{"metadata":{"trusted":true},"cell_type":"code","source":"from tqdm.auto import tqdm","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def classify(model, test_img_dir):\n\n    result = dict()\n    keys = ['name', *['c%d' % i for i in range(10)]]\n    for key in keys:\n        result[key] = []\n        \n    paths = sorted(list(os.listdir(test_img_dir)))\n\n    for path in tqdm(paths):\n        if not path.endswith('.jpg'):\n            continue\n        \n        image = cv2.imread(os.path.join(test_img_dir, path), cv2.IMREAD_COLOR)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        image = cv2.resize(image, (INPUT_SHAPE, INPUT_SHAPE), interpolation = cv2.INTER_CUBIC)\n\n        pred = model.predict(np.expand_dims(image, 0))[0]\n        result['name'].append(path)\n        for i in range(10):\n            result['c%d' % i].append(pred[i])\n\n    return result","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = load_model('model1_stage2.hdf5')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"result = classify(model, TEST_DIR)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dct = {'img': result['name']}\nfor i in range(10):\n    col = 'c%d' % i\n    dct[col] = result[col]\n    \ndf = pd.DataFrame(dct)\ndf.to_csv('submission.csv', index=False)\ndf","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Improve score a bit with clipping hack:"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_new = df.copy()\n\ncols = ['c0', 'c1', 'c2', 'c3', 'c4', 'c5', 'c6', 'c7', 'c8', 'c9']\narr, names = df_new.values[:, 1:], df_new.values[:, 0]\narr = np.clip(arr, 0.025, 0.975)\narr /= np.sum(arr, axis=1, keepdims=True)\n\nnew_df = pd.DataFrame(np.hstack([names.reshape((-1, 1)), arr]), columns=df.columns)\nnew_df.to_csv('submission_new.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Extract logits on train and val for distillation:"},{"metadata":{"trusted":true},"cell_type":"code","source":"batch_ind, num_batches = None, None","execution_count":17,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class PredictDataGenerator(tf.keras.utils.Sequence):\n    def __init__(self, data, batch_size, augmentation, image_size):\n        self.batch_size = batch_size\n        self.augmentation = augmentation\n        self.image_size = image_size\n\n        self.image_names = np.array([item['path'] for item in data])\n        self.targets = to_categorical(np.array([int(item['label'][1:]) for item in data]))\n        self.samples = len(self.targets)\n\n        self.indexes = np.arange(self.samples)\n\n    def __len__(self):\n        return int(np.ceil(self.samples / self.batch_size))\n\n    def transform_image(self, image):\n        image = cv2.resize(image, (self.image_size, self.image_size), interpolation = cv2.INTER_CUBIC)\n\n        if self.augmentation:\n            image = self.augmentation(image=image)['image']\n\n        return image\n\n    def __getitem__(self, index):\n        global batch_ind, num_batches\n        if batch_ind % 10 == 0:\n            print(f\"{batch_ind + 1} / {num_batches}\")\n        batch_ind += 1\n        \n        take_ind = self.indexes[index * self.batch_size: min((index + 1) * self.batch_size, len(self.targets))]\n        X = np.empty((len(take_ind), self.image_size, self.image_size, 3))\n        y = self.targets[take_ind, :]\n\n        for i in range(len(take_ind)):\n            img = cv2.imread(self.image_names[take_ind[i]], cv2.IMREAD_COLOR)\n            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n            img = self.transform_image(img)\n            X[i] = img\n\n        return X, y","execution_count":42,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import gdown\n\nurl = 'https://drive.google.com/u/0/uc?id=1iMZeYCX-Qijk4iPL5AULQvt21LroUw3K&export=download'\noutput = 'teacher.zip'\ngdown.download(url, output, quiet=False)\n!unzip teacher.zip\n\nteacher = load_model('model1_stage2.hdf5')\n\nteacher_copy= tf.keras.models.clone_model(teacher)\nteacher_copy.build(teacher.input_shape)\nteacher_copy.set_weights(teacher.get_weights())\n\nx = teacher_copy.layers[-2].output\npredictions = Dense(num_classes, activation='linear')(x)\nteacher_copy = Model(inputs=teacher_copy.input, outputs=predictions)\nteacher_copy.layers[-1].set_weights(teacher.layers[-1].get_weights())\n\nmodel = teacher_copy\nmodel._name = 'teacher'","execution_count":43,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pickle\n\nBATCH_SIZE = 32\nINPUT_SHAPE = 380\n\npredict_train_generator = PredictDataGenerator(train_data, BATCH_SIZE, None, INPUT_SHAPE)\n\nbatch_ind, num_batches = 0, np.ceil(predict_train_generator.samples / BATCH_SIZE).astype(int)\n\nfilenames = predict_train_generator.image_names\npredict = model.predict_generator(predict_train_generator, steps=num_batches)\n\ntrain_pred = (filenames, predict)\npickle.dump(train_pred, open('train_pred', 'wb'))","execution_count":45,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pickle\n\nBATCH_SIZE = 32\nINPUT_SHAPE = 380\n\npredict_val_generator = PredictDataGenerator(val_data, BATCH_SIZE, None, INPUT_SHAPE)\n\nbatch_ind, num_batches = 0, np.ceil(predict_val_generator.samples / BATCH_SIZE).astype(int)\n\nfilenames = predict_val_generator.image_names\npredict = model.predict_generator(predict_val_generator, steps=num_batches)\n\nval_pred = (filenames, predict)\npickle.dump(val_pred, open('val_pred', 'wb'))","execution_count":46,"outputs":[{"output_type":"stream","text":"1 / 141\n11 / 141\n21 / 141\n31 / 141\n41 / 141\n51 / 141\n61 / 141\n71 / 141\n81 / 141\n91 / 141\n101 / 141\n111 / 141\n121 / 141\n131 / 141\n141 / 141\n","name":"stdout"}]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}